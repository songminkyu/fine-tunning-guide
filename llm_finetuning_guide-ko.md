# LLM Fine-tuning Guide 2025 π€

> 2025λ…„ ν„μ¬ κ°€μ¥ μ‹¤μ©μ μΈ LLM νμΈνλ‹ λ°©λ²•λ΅ κ³Ό κΈ°λ²•λ“¤μ„ μ •λ¦¬ν• μ™„μ „ κ°€μ΄λ“

## π“‹ λ©μ°¨

- [κ°μ”](#κ°μ”)
- [ν•µμ‹¬ κ°λ… μ΄ν•΄](#ν•µμ‹¬-κ°λ…-μ΄ν•΄)
- [νμΈνλ‹ λ°©λ²•λ΅ ](#νμΈνλ‹-λ°©λ²•λ΅ )
- [ν¨μ¨μ„± κΈ°λ²• (PEFT)](#ν¨μ¨μ„±-κΈ°λ²•-peft)
- [μ‹¤λ¬΄ κ°€μ΄λ“λΌμΈ](#μ‹¤λ¬΄-κ°€μ΄λ“λΌμΈ)
- [λ‹¨κ³„λ³„ ν•™μµ κ²½λ΅](#λ‹¨κ³„λ³„-ν•™μµ-κ²½λ΅)
- [μƒν™©λ³„ μ„ νƒ κ°€μ΄λ“](#μƒν™©λ³„-μ„ νƒ-κ°€μ΄λ“)
- [μ‹¤μ  κµ¬ν„ μμ‹](#μ‹¤μ -κµ¬ν„-μμ‹)
- [μ°Έκ³  μλ£](#μ°Έκ³ -μλ£)

## κ°μ”

Large Language Model (LLM) νμΈνλ‹μ€ μ‚¬μ „ ν›λ ¨λ λ¨λΈμ„ νΉμ • μ‘μ—…μ΄λ‚ λ„λ©”μΈμ— λ§κ² μ΅°μ •ν•λ” κ³Όμ •μ…λ‹λ‹¤. μ΄ κ°€μ΄λ“λ” 2025λ…„ ν„μ¬ μ‹¤λ¬΄μ—μ„ κ°€μ¥ λ§μ΄ μ‚¬μ©λλ” λ°©λ²•λ΅ λ“¤μ„ μ²΄κ³„μ μΌλ΅ μ •λ¦¬ν•©λ‹λ‹¤.

### π― μ΄ κ°€μ΄λ“μ λ©ν‘

- λ³µμ΅ν• νμΈνλ‹ κΈ°λ²•λ“¤μ„ λ…ν™•ν•κ² λ¶„λ¥
- μ‹¤λ¬΄μ—μ„ λ°”λ΅ μ μ© κ°€λ¥ν• κ°€μ΄λ“λΌμΈ μ κ³µ
- μƒν™©λ³„ μµμ  λ°©λ²• μ„ νƒ λ„μ›€

## ν•µμ‹¬ κ°λ… μ΄ν•΄

### νμΈνλ‹μ 2μ°¨μ› κµ¬μ΅°

```
LLM νμΈνλ‹ = ν•™μµ λ©μ (What) Γ— ν¨μ¨μ„± λ°©λ²•(How)
```

#### π“ ν•™μµ λ©μ  (What to learn)
- **SFT (Supervised Fine-tuning)**: μ§€λ„ν•™μµμΌλ΅ νΉμ • μ‘μ—… ν•™μµ
- **RLHF (Reinforcement Learning from Human Feedback)**: μΈκ°„ ν”Όλ“λ°± κΈ°λ° κ°•ν™”ν•™μµ
- **DPO (Direct Preference Optimization)**: μ§μ ‘ μ„ νΈλ„ μµμ ν™”

#### β΅ ν¨μ¨μ„± λ°©λ²• (How to train)
- **Full Fine-tuning**: λ¨λ“  νλΌλ―Έν„° μ—…λ°μ΄νΈ
- **PEFT (Parameter Efficient Fine-tuning)**: μΌλ¶€ νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈ

## νμΈνλ‹ λ°©λ²•λ΅ 

### 1. SFT (Supervised Fine-tuning)

**κ°λ…**: μ‚¬μ „ ν›λ ¨λ λ¨λΈμ„ λΌλ²¨λ§λ λ°μ΄ν„°λ΅ μ§€λ„ν•™μµ

**νΉμ§•**:
- κ°€μ¥ κΈ°λ³Έμ μ΄κ³  μ•μ •μ μΈ λ°©λ²•
- Instruction-following λ¥λ ¥ ν–¥μƒ
- λΉ„κµμ  μ μ€ λ°μ΄ν„°λ΅λ„ ν¨κ³Όμ 

**μ‚¬μ© μ‹κΈ°**: 
- νΉμ • λ„λ©”μΈ μ μ‘
- κΈ°λ³Έμ μΈ λ€ν™” λ¥λ ¥ λ¶€μ—¬
- μ‘μ—…λ³„ μ„±λ¥ ν–¥μƒ

### 2. RLHF + PPO

**κ°λ…**: μΈκ°„ ν”Όλ“λ°±μ„ ν†µν• κ°•ν™”ν•™μµ

**νΉμ§•**:
- μΈκ°„μ μ„ νΈλ„λ¥Ό λ¨λΈμ— λ°μ
- λ³µμ΅ν•κ³  κ³„μ‚° λΉ„μ©μ΄ λ†’μ
- μ•μ •μ„± λ¬Έμ  μ΅΄μ¬

**μ‚¬μ© μ‹κΈ°**:
- κ³ ν’μ§ λ€ν™”ν• AI κµ¬μ¶•
- μ•μ „μ„±μ΄ μ¤‘μ”ν• μ• ν”λ¦¬μΌ€μ΄μ…

### 3. DPO (Direct Preference Optimization)

**κ°λ…**: RLHFλ¥Ό λ‹¨μν™”ν• μ§μ ‘ μ„ νΈλ„ μµμ ν™”

**νΉμ§•**:
- RLHFλ³΄λ‹¤ κ°„λ‹¨ν•κ³  μ•μ •μ 
- λ³„λ„ λ³΄μƒ λ¨λΈ λ¶ν•„μ”
- 2023λ…„ Stanfordμ—μ„ λ„μ…

**μ‚¬μ© μ‹κΈ°**:
- RLHFμ λ³µμ΅μ„±μ„ ν”Όν•κ³  μ‹¶μ„ λ•
- μ„ νΈλ„ λ°μ΄ν„°κ°€ μ¶©λ¶„ν•  λ•

## ν¨μ¨μ„± κΈ°λ²• (PEFT)

### PEFTλ€?

**Parameter Efficient Fine-Tuning**μ μ¤„μ„λ§λ΅, μ „μ²΄ νλΌλ―Έν„° λ€μ‹  μ†μμ νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈν•μ—¬ λ©”λ¨λ¦¬μ™€ μ—°μ‚°μ„ μ μ•½ν•λ” κΈ°λ²•λ“¤μ μ΄μΉ­

### μ£Όμ” PEFT κΈ°λ²•λ“¤

#### 1. LoRA (Low-Rank Adaptation)

```
π’΅ ν•µμ‹¬: κ°€μ¤‘μΉ ν–‰λ ¬μ„ λ‘ κ°μ μ‘μ€ ν–‰λ ¬λ΅ λ¶„ν•΄
```

**νΉμ§•**:
- μ „μ²΄ νλΌλ―Έν„°μ 0.5-5%λ§ μ—…λ°μ΄νΈ
- λ©”λ¨λ¦¬ μ‚¬μ©λ‰ 70% κ°μ†
- μ–΄λ‘ν„° νμΌ ν¬κΈ°κ°€ λ§¤μ° μ‘μ (λ‡ MB)

**μ¥μ **:
- μ—¬λ¬ LoRA μ–΄λ‘ν„° κµμ²΄ κ°€λ¥
- μ›λ³Έ λ¨λΈ κ°€μ¤‘μΉ λ³΄μ΅΄
- λΉ λ¥Έ ν›λ ¨κ³Ό μ¶”λ΅ 

#### 2. QLoRA (Quantized LoRA)

```
π’΅ ν•µμ‹¬: LoRA + 4-bit μ–‘μν™”
```

**νΉμ§•**:
- 4-bit NormalFloat (NF4) μ–‘μν™” μ‚¬μ©ΒΉ
- 65B νλΌλ―Έν„° λ¨λΈμ„ λ‹¨μΌ 48GB GPUμ—μ„ ν›λ ¨ κ°€λ¥Β²
- LoRAλ³΄λ‹¤ 33% λ©”λ¨λ¦¬ μ μ•½, 39% ν›λ ¨ μ‹κ°„ μ¦κ°€Β³

**νμ‹ μ  κΈ°μ **:
- Double Quantizationβ΄
- Paged Optimizers
- μ–‘μν™” μ¤λ¥λ¥Ό LoRAλ΅ λ³΄μ •

---
ΒΉΒ² [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)  
Β³ [Practical LoRA Tips](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)  
β΄ [QLoRA Technical Details](https://www.analyticsvidhya.com/blog/2023/08/lora-and-qlora/)

#### 3. κΈ°νƒ€ PEFT κΈ°λ²•

- **Adapter Tuning**: κ° λ μ΄μ–΄μ— μ‘μ€ μ–΄λ‘ν„° λ¨λ“ μ¶”κ°€
- **Prefix Tuning**: μ…λ ¥μ— ν•™μµ κ°€λ¥ν• prefix μ¶”κ°€
- **P-Tuning**: μ—°μ†μ μΈ prompts ν•™μµ

## μ‹¤λ¬΄ κ°€μ΄λ“λΌμΈ

### π― μ΄λ³΄μ μ¶”μ² κ²½λ΅

```
1λ‹¨κ³„: SFT + LoRA (κΈ°λ³ΈκΈ° μµνκΈ°)
2λ‹¨κ³„: SFT + QLoRA (λ©”λ¨λ¦¬ μµμ ν™”)
3λ‹¨κ³„: DPO + LoRA (μ„±λ¥ ν–¥μƒ)
```

### π“ 2025λ…„ μ‹¤λ¬΄ νΈλ λ“

**μ£Όλ¥ λ°©λ²•λ΅ ** (μ¶μ²: 2025λ…„ μ—…κ³„ λ¶„μ„ λ³΄κ³ μ„λ“¤):
- SFT + QLoRA/LoRA μ΅°ν•©μ΄ λ„λ¦¬ μ‚¬μ©λ¨ΒΉ
- DPOκ°€ RLHFμ κ°•λ ¥ν• λ€μ•μΌλ΅ λ¶€μƒΒ²
- Full Fine-tuningμ€ νΉμν• κ²½μ°μ—λ§ μ‚¬μ©Β³

**μ„±λ¥ κ°μ„  μ‚¬λ΅€**:
- QLoRAλ” λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ 33% μ μ•½ν•λ©΄μ„ μ„±λ¥ μ†μ‹¤ μµμ†ν™”β΄
- LoRAλ” μ „μ²΄ νμΈνλ‹ λ€λΉ„ 0.2-0.3%μ νλΌλ―Έν„°λ§μΌλ΅ κ²½μλ ¥ μλ” μ„±λ¥βµ

---
ΒΉ [LLM Fine-Tuning Architecture 2025](https://sam-solutions.com/blog/llm-fine-tuning-architecture/)  
Β² [DPO vs RLHF Analysis](https://medium.com/@bavalpreetsinghh/rlhf-ppo-vs-dpo-26b1438cf22b)  
Β³ [Fine-tuning Landscape 2025](https://medium.com/@pradeepdas/the-fine-tuning-landscape-in-2025-a-comprehensive-analysis-d650d24bed97)  
β΄ [QLoRA Performance Study](https://lightning.ai/pages/community/lora-insights/)  
βµ [LoRA Efficiency Analysis](https://www.geeksforgeeks.org/deep-learning/fine-tuning-using-lora-and-qlora/)

## λ‹¨κ³„λ³„ ν•™μµ κ²½λ΅

### Phase 1: κΈ°λ³Έ νμΈνλ‹

```bash
Base Model β†’ SFT β†’ ν‰κ°€
```

**λ©ν‘**: κΈ°λ³Έμ μΈ instruction-following λ¥λ ¥ μµλ“

### Phase 2: μ„±λ¥ μµμ ν™”

```bash
SFT Model β†’ DPO/RLHF β†’ ν‰κ°€
```

**λ©ν‘**: μΈκ°„ μ„ νΈλ„μ— λ§λ” κ³ ν’μ§ μ‘λ‹µ μƒμ„±

### Phase 3: ν¨μ¨μ„± μµμ ν™”

```bash
λ¨λ“  λ‹¨κ³„μ—μ„ PEFT κΈ°λ²• μ μ©
```

**λ©ν‘**: λ¦¬μ†μ¤ ν¨μ¨μ μΈ ν›λ ¨ λ° λ°°ν¬

## μƒν™©λ³„ μ„ νƒ κ°€μ΄λ“

### π”§ ν•λ“μ›¨μ–΄ μ΅°κ±΄λ³„

| GPU λ©”λ¨λ¦¬ | λ¨λΈ ν¬κΈ° | μ¶”μ² λ°©λ²• |
|-----------|----------|----------|
| 8GB μ΄ν• | 7B μ΄ν• | SFT + QLoRA |
| 16-24GB | 7B-13B | SFT + LoRA |
| 24GB+ | 13B+ | SFT + LoRA/QLoRA |
| 80GB+ | 70B+ | Full Fine-tuning κ°€λ¥ |

### π― λ©μ λ³„

| λ©μ  | μ¶”μ² λ°©λ²• | νΉμ§• |
|------|----------|------|
| λ„λ©”μΈ νΉν™” | SFT + LoRA | μ•μ •μ , λΉ λ¦„ |
| λ€ν™”ν• AI | SFT β†’ DPO | λ†’μ€ ν’μ§ |
| λΉ λ¥Έ ν”„λ΅ν† νƒ€μ΄ν•‘ | SFT + QLoRA | μµμ† λ¦¬μ†μ¤ |
| ν”„λ΅λ•μ… λ°°ν¬ | DPO + LoRA | κ· ν•μ΅ν μ„±λ¥ |

### π’Ύ λ°μ΄ν„° μ΅°κ±΄λ³„

| λ°μ΄ν„° ν¬κΈ° | ν’μ§ | μ¶”μ² λ°©λ²• |
|------------|------|----------|
| μ†λ‰ (<1K) | κ³ ν’μ§ | Few-shot + LoRA |
| μ¤‘κ°„ (1K-10K) | μ¤‘κ°„ | SFT + LoRA |
| λ€λ‰ (10K+) | νΌμ¬ | SFT β†’ DPO |

## μ‹¤μ  κµ¬ν„ μμ‹

### SFT + LoRA κΈ°λ³Έ μμ‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer

# λ¨λΈ λ΅λ“
model = AutoModelForCausalLM.from_pretrained("model_name")
tokenizer = AutoTokenizer.from_pretrained("model_name")

# LoRA μ„¤μ •
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
)

# PEFT λ¨λΈ μƒμ„±
model = get_peft_model(model, lora_config)

# SFT ν›λ ¨
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    tokenizer=tokenizer,
)

trainer.train()
```

### QLoRA μ„¤μ • μμ‹

```python
import torch
from transformers import BitsAndBytesConfig

# QLoRAλ¥Ό μ„ν• μ–‘μν™” μ„¤μ •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# λ¨λΈ λ΅λ“ (4-bit)
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    quantization_config=bnb_config,
    device_map="auto"
)
```

## ν•μ΄νΌνλΌλ―Έν„° κ°€μ΄λ“

### LoRA ν•μ΄νΌνλΌλ―Έν„°

| νλΌλ―Έν„° | μ¶”μ² κ°’ | μ„¤λ… |
|---------|--------|------|
| r (rank) | 8-64 | λ‚®μ„μλ΅ νλΌλ―Έν„° μ μ |
| alpha | r*2 | ν•™μµλ¥  μ¤μΌ€μΌλ§ |
| dropout | 0.05-0.1 | κ³Όμ ν•© λ°©μ§€ |

### ν•™μµ ν•μ΄νΌνλΌλ―Έν„°

| νλΌλ―Έν„° | μ¶”μ² κ°’ | μ„¤λ… |
|---------|--------|------|
| Learning Rate | 1e-4 ~ 5e-4 | LoRA: λ†’κ², Full: λ‚®κ² |
| Batch Size | 4-16 | GPU λ©”λ¨λ¦¬μ— λ”°λΌ μ΅°μ • |
| Epochs | 1-3 | κ³Όμ ν•© μ£Όμ |

## μ„±λ¥ μµμ ν™” ν

### π€ ν›λ ¨ μ†λ„ ν–¥μƒ

- **Gradient Checkpointing**: λ©”λ¨λ¦¬ vs μ†λ„ νΈλ μ΄λ“μ¤ν”„
- **Mixed Precision**: bfloat16 μ‚¬μ©
- **DataLoader**: num_workers μ΅°μ •

### π’Ύ λ©”λ¨λ¦¬ μµμ ν™”

- **Gradient Accumulation**: ν° λ°°μΉ μ‚¬μ΄μ¦ ν¨κ³Ό
- **DeepSpeed ZeRO**: λ€ν• λ¨λΈμ©
- **Model Sharding**: λ‹¤μ¤‘ GPU ν™μ©

### π“ μ„±λ¥ λ¨λ‹ν„°λ§

- **Loss Curves**: κ³Όμ ν•© νƒμ§€
- **Validation Metrics**: μΌλ°ν™” μ„±λ¥
- **Resource Usage**: GPU/CPU μ‚¬μ©λ¥ 

## λ¬Έμ  ν•΄κ²° κ°€μ΄λ“

### μμ£Ό λ°μƒν•λ” λ¬Έμ λ“¤

#### 1. Out of Memory (OOM)

**ν•΄κ²°μ±…**:
- QLoRA μ‚¬μ©
- Batch size μ¤„μ΄κΈ°
- Gradient checkpointing ν™μ„±ν™”
- λ” μ‘μ€ λ¨λΈ μ‚¬μ©

#### 2. μ„±λ¥ μ €ν•

**ν•΄κ²°μ±…**:
- Learning rate μ΅°μ •
- λ” λ§μ€ λ°μ΄ν„° μμ§‘
- ν•μ΄νΌνλΌλ―Έν„° νλ‹
- λ‹¤λ¥Έ PEFT κΈ°λ²• μ‹λ„

#### 3. ν›λ ¨ λ¶μ•μ •

**ν•΄κ²°μ±…**:
- Learning rate scheduler μ‚¬μ©
- Warmup steps μ¶”κ°€
- Gradient clipping μ μ©

## ν‰κ°€ λ°©λ²•λ΅ 

### μλ™ ν‰κ°€ μ§€ν‘

- **Perplexity**: μ–Έμ–΄ λ¨λΈλ§ μ„±λ¥
- **BLEU/ROUGE**: μƒμ„± ν’μ§
- **Task-specific metrics**: μ‘μ—…λ³„ μ§€ν‘

### μΈκ°„ ν‰κ°€

- **Helpfulness**: λ„μ›€μ΄ λλ” μ •λ„
- **Harmlessness**: μ•μ „μ„±
- **Honesty**: μ •ν™•μ„±

## λ°°ν¬ κ³ λ ¤μ‚¬ν•­

### λ¨λΈ ν¬κΈ° μµμ ν™”

- **LoRA Adapter**: λ‡ MBλ΅ λ§¤μ° μ‘μ
- **Model Merging**: λ°°ν¬μ‹ μ–΄λ‘ν„° λ³‘ν•©
- **Quantization**: μ¶”λ΅ μ‹ μ–‘μν™”

### μ¶”λ΅  μµμ ν™”

- **Batch Processing**: μ²λ¦¬λ‰ ν–¥μƒ
- **Caching**: λ°λ³µ κ³„μ‚° λ°©μ§€
- **Hardware Acceleration**: GPU/TPU ν™μ©

## μµμ‹  νΈλ λ“ (2025λ…„)

### π”¥ Hot Topics

- **Multimodal Fine-tuning**: ν…μ¤νΈ+μ΄λ―Έμ§€+μ¤λ””μ¤
- **Domain-specific LLMs**: νΉν™” λ¨λΈλ“¤
- **Autonomous Agents**: μ—μ΄μ „νΈν• AI
- **Small Language Models**: κ²½λ‰ν™” λ¨λΈ

### μƒλ΅μ΄ κΈ°λ²•λ“¤

- **Flash Attention**: κΈ΄ μ‹ν€€μ¤ μ²λ¦¬
- **RTO (Reinforced Token Optimization)**: PPO + DPO κ²°ν•©
- **Mixed Preference Optimization**: DPO β†’ PPO 2λ‹¨κ³„

## μ°Έκ³  μλ£

### π“ ν•„μ λ…Όλ¬Έ

- **LoRA**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- **QLoRA**: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- **DPO**: [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
- **InstructGPT**: [Training language models to follow instructions](https://arxiv.org/abs/2203.02155)
- **PPO**: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)

### π“„ μ£Όμ” κΈ°μ  λ¬Έμ„ λ° λΈ”λ΅κ·Έ

#### 2025λ…„ μµμ‹  νΈλ λ“ λ¶„μ„
- [The Fine-Tuning Landscape in 2025: A Comprehensive Analysis](https://medium.com/@pradeepdas/the-fine-tuning-landscape-in-2025-a-comprehensive-analysis-d650d24bed97)
- [Fine-tuning large language models (LLMs) in 2025 | SuperAnnotate](https://www.superannotate.com/blog/llm-fine-tuning)
- [LLM Fine Tuning: The 2025 Guide for ML Teams](https://labelyourdata.com/articles/llm-fine-tuning)

#### LoRA & QLoRA μƒμ„Έ κ°€μ΄λ“
- [Efficient Fine-Tuning with LoRA: A Guide to Optimal Parameter Selection](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms)
- [Parameter-Efficient Fine-Tuning of Large Language Models with LoRA and QLoRA](https://www.analyticsvidhya.com/blog/2023/08/lora-and-qlora/)
- [Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments](https://lightning.ai/pages/community/lora-insights/)
- [Practical Tips for Finetuning LLMs Using LoRA](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)

#### DPO & RLHF μ‹¬ν™” μλ£
- [What is direct preference optimization (DPO)? | SuperAnnotate](https://www.superannotate.com/blog/direct-preference-optimization-dpo)
- [RLHF(PPO) vs DPO](https://medium.com/@bavalpreetsinghh/rlhf-ppo-vs-dpo-26b1438cf22b)
- [Navigating the RLHF Landscape: From Policy Gradients to PPO, GAE, and DPO](https://huggingface.co/blog/NormalUhr/rlhf-pipeline)
- [Rethinking the Role of PPO in RLHF β€“ Berkeley AI Research](https://bair.berkeley.edu/blog/2023/10/16/p3o/)

#### μ‹¤λ¬΄ κµ¬ν„ κ°€μ΄λ“
- [LLM Fine-Tuning Architecture: Methods, Best Practices & Challenges](https://sam-solutions.com/blog/llm-fine-tuning-architecture/)
- [Fine-Tuning using LoRA and QLoRA - GeeksforGeeks](https://www.geeksforgeeks.org/deep-learning/fine-tuning-using-lora-and-qlora/)
- [Understanding and Using Supervised Fine-Tuning (SFT) for Language Models](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

### π› οΈ λΌμ΄λΈλ¬λ¦¬ & λ„κµ¬

- **[Hugging Face Transformers](https://github.com/huggingface/transformers)**: κΈ°λ³Έ λ¨λΈ λΌμ΄λΈλ¬λ¦¬
- **[PEFT](https://github.com/huggingface/peft)**: Parameter Efficient Fine-tuning
- **[TRL](https://github.com/huggingface/trl)**: Transformer Reinforcement Learning
- **[DeepSpeed](https://github.com/microsoft/DeepSpeed)**: λ€κ·λ¨ λ¨λΈ ν›λ ¨
- **[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)**: μ–‘μν™” λΌμ΄λΈλ¬λ¦¬
- **[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)**: ν†µν•© νμΈνλ‹ ν΄ν‚·
- **[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)**: μ‰¬μ΄ LLM νμΈνλ‹

### π μ μ©ν• λ§ν¬

- [Hugging Face Model Hub](https://huggingface.co/models)
- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [Papers With Code - Fine-tuning](https://paperswithcode.com/task/fine-tuning)

### π“ λ°μ΄ν„°μ…‹

- **Alpaca**: [Stanford Alpaca Dataset](https://github.com/tatsu-lab/stanford_alpaca)
- **Vicuna**: [Vicuna Training Dataset](https://github.com/lm-sys/FastChat)
- **UltraChat**: [UltraChat 200k Dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)
- **OpenAssistant**: [OASST1 Dataset](https://huggingface.co/datasets/OpenAssistant/oasst1)
- **LLaVA Instruct**: [LLaVA Instruction Mix](https://huggingface.co/datasets/trl-lib/llava-instruct-mix)

## μ»¤λ®¤λ‹ν‹° & μ§€μ›

### π’¬ ν™λ°ν• μ»¤λ®¤λ‹ν‹°

- [r/MachineLearning](https://reddit.com/r/MachineLearning)
- [Hugging Face Forums](https://discuss.huggingface.co/)
- [Papers With Code](https://paperswithcode.com/)

### π† λ„μ›€ λ°›κΈ°

- GitHub Issues (κ° λΌμ΄λΈλ¬λ¦¬)
- Stack Overflow
- Discord μ»¤λ®¤λ‹ν‹°λ“¤

---

## β οΈ μ£Όμμ‚¬ν•­

1. **μ €μ‘κ¶**: λ°μ΄ν„°μ…‹κ³Ό λ¨λΈμ λΌμ΄μ„ μ¤ ν™•μΈ
2. **μ»΄ν“¨ν… λΉ„μ©**: ν΄λΌμ°λ“ μ‚¬μ©μ‹ λΉ„μ© λ¨λ‹ν„°λ§
3. **μ•μ „μ„±**: νΈν–¥μ„±κ³Ό μ ν•΄μ„± κ²€ν† 
4. **μ„±λ¥**: μ‹¤μ  μ‚¬μ© μΌ€μ΄μ¤μ—μ„ μ¶©λ¶„ν• ν…μ¤νΈ

## μ£Όμ” μΈμ© λ° μ¶μ²

μ΄ κ°€μ΄λ“λ” λ‹¤μ μ‹ λΆ°ν•  μ μλ” μ¶μ²λ“¤μ„ λ°”νƒ•μΌλ΅ μ‘μ„±λμ—μµλ‹λ‹¤:

### π”¬ ν•™μ  λ…Όλ¬Έ λ° μ—°κµ¬

1. **QLoRA μ›λ΅ **: Dettmers, T., et al. (2023). "QLoRA: Efficient Finetuning of Quantized LLMs" - 65B νλΌλ―Έν„° λ¨λΈμ„ λ‹¨μΌ 48GB GPUμ—μ„ νμΈνλ‹ κ°€λ¥ν•¨μ„ μ…μ¦
2. **LoRA κΈ°λ²•**: Hu, E., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models" - νλΌλ―Έν„° ν¨μ¨μ  νμΈνλ‹μ κΈ°μ΄
3. **DPO λ°©λ²•λ΅ **: Rafailov, R., et al. (2023). "Direct Preference Optimization" - RLHFμ μ•μ •μ  λ€μ• μ μ‹

### π“ μ‚°μ—… λ™ν–¥ λ¶„μ„

- **2025λ…„ νμΈνλ‹ νΈλ λ“**: Medium/@pradeepdasμ μΆ…ν•© λ¶„μ„ λ³΄κ³ μ„
- **μ‹¤λ¬΄ μ μ© μ‚¬λ΅€**: SuperAnnotate, DataCamp, Lightning AIμ κΈ°μ  λΈ”λ΅κ·Έ
- **μ„±λ¥ λ²¤μΉλ§ν¬**: GeeksforGeeks, Analytics Vidhyaμ μ‹¤ν— κ²°κ³Ό

### πΆ κΈ°μ—… κΈ°μ  λ¬Έμ„

- **Hugging Face**: κ³µμ‹ TRL, PEFT λΌμ΄λΈλ¬λ¦¬ λ¬Έμ„
- **Microsoft**: DeepSpeed μµμ ν™” κ°€μ΄λ“  
- **Databricks**: LoRA μµμ ν™” μ‹¤ν— κ²°κ³Ό
- **Berkeley AI Research**: PPO vs P3O μ„±λ¥ λΉ„κµ

---

**β οΈ λ©΄μ±…μ‚¬ν•­**: 
- λ¨λ“  κΈ°μ μ  μ£Όμ¥μ€ λ…μ‹λ μ¶μ²λ¥Ό λ°”νƒ•μΌλ΅ ν•©λ‹λ‹¤
- μ„±λ¥ μμΉλ” νΉμ • μ‹¤ν— ν™κ²½μ—μ„ μΈ΅μ •λ κ²ƒμΌλ΅, ν™κ²½μ— λ”°λΌ λ‹¤λ¥Ό μ μμµλ‹λ‹¤
- κΈ‰μ†ν λ°μ „ν•λ” λ¶„μ•Ό νΉμ„±μƒ μµμ‹  μ •λ³΄ ν™•μΈμ„ κ¶μ¥ν•©λ‹λ‹¤

---

**π“ λΌμ΄μ„ μ¤**: MIT License

**π¤ κΈ°μ—¬**: Pull Requests ν™μν•©λ‹λ‹¤!

**π“§ λ¬Έμ**: [μ΄μ μƒμ„±](../../issues) λλ” ν† λ΅  κ²μ‹ν ν™μ©

**π”„ μ—…λ°μ΄νΈ**: μ΄ λ¬Έμ„λ” μ •κΈ°μ μΌλ΅ μ—…λ°μ΄νΈλ©λ‹λ‹¤. [Watch](../../watchers) μ„¤μ •μΌλ΅ λ³€κ²½μ‚¬ν•­μ„ λ°›μ•„λ³΄μ„Έμ”.

---

*μ΄ κ°€μ΄λ“λ” 2025λ…„ 1μ›” κΈ°μ¤€μΌλ΅ μ‘μ„±λμ—μΌλ©°, λΉ λ¥΄κ² λ°μ „ν•λ” λ¶„μ•Όμ νΉμ„±μƒ μ •κΈ°μ μΈ μ—…λ°μ΄νΈκ°€ ν•„μ”ν•©λ‹λ‹¤.*

### μ°Έκ³ 
https://github.com/songminkyu/fine-tunning